{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "grave-occasions",
   "metadata": {
    "id": "TZSL793i7KuM",
    "papermill": {
     "duration": 0.009793,
     "end_time": "2021-02-09T19:20:12.335229",
     "exception": false,
     "start_time": "2021-02-09T19:20:12.325436",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "theoretical-deployment",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-09T19:20:12.362930Z",
     "iopub.status.busy": "2021-02-09T19:20:12.362193Z",
     "iopub.status.idle": "2021-02-09T19:20:14.041282Z",
     "shell.execute_reply": "2021-02-09T19:20:14.041708Z"
    },
    "papermill": {
     "duration": 1.697233,
     "end_time": "2021-02-09T19:20:14.041904",
     "exception": false,
     "start_time": "2021-02-09T19:20:12.344671",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install python-dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "bucket = os.getenv(\"BUCKET\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "environmental-reduction",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-09T19:20:14.067942Z",
     "iopub.status.busy": "2021-02-09T19:20:14.067133Z",
     "iopub.status.idle": "2021-02-09T19:20:14.069054Z",
     "shell.execute_reply": "2021-02-09T19:20:14.069512Z"
    },
    "id": "hVPzEKoLuEHy",
    "papermill": {
     "duration": 0.017454,
     "end_time": "2021-02-09T19:20:14.069706",
     "exception": false,
     "start_time": "2021-02-09T19:20:14.052252",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_TRAIN_STEPS = 1\n",
    "MODEL_TYPE = 'ssd_mobilenet_v1_quantized_300x300_coco14_sync_2018_07_18'\n",
    "CONFIG_TYPE = 'ssd_mobilenet_v1_quantized_300x300_coco14_sync'\n",
    "\n",
    "import os\n",
    "CLOUD_ANNOTATIONS_MOUNT = os.path.join('tmp', bucket)\n",
    "ANNOTATIONS_JSON_PATH   = os.path.join(CLOUD_ANNOTATIONS_MOUNT, '_annotations.json')\n",
    "\n",
    "CHECKPOINT_PATH = 'tmp/checkpoint'\n",
    "OUTPUT_PATH     = 'tmp/output'\n",
    "EXPORTED_PATH   = 'tmp/exported'\n",
    "DATA_PATH       = 'tmp/data'\n",
    "\n",
    "LABEL_MAP_PATH    = os.path.join(DATA_PATH, 'label_map.pbtxt')\n",
    "TRAIN_RECORD_PATH = os.path.join(DATA_PATH, 'train.record')\n",
    "VAL_RECORD_PATH   = os.path.join(DATA_PATH, 'val.record')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protecting-shipping",
   "metadata": {
    "id": "3XINCKkPshgz",
    "papermill": {
     "duration": 0.009653,
     "end_time": "2021-02-09T19:20:14.088664",
     "exception": false,
     "start_time": "2021-02-09T19:20:14.079011",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Install the TensorFlow Object Detection API\n",
    "In order to use the TensorFlow Object Detection API, we need to clone it's GitHub Repo.\n",
    "\n",
    "### Dependencies\n",
    "Most of the dependencies required come preloaded in Google Colab. The only additional package we need to install is TensorFlow.js, which is used for converting our trained model to a model that is compatible for the web.\n",
    "\n",
    "### Protocol Buffers\n",
    "The TensorFlow Object Detection API relies on what are called `protocol buffers` (also known as `protobufs`). Protobufs are a language neutral way to describe information. That means you can write a protobuf once and then compile it to be used with other languages, like Python, Java or C.\n",
    "\n",
    "The `protoc` command used below is compiling all the protocol buffers in the `object_detection/protos` folder for Python.\n",
    "\n",
    "### Environment\n",
    "To use the object detection api we need to add it to our `PYTHONPATH` along with `slim` which contains code for training and evaluating several widely used Convolutional Neural Network (CNN) image classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statutory-religion",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-09T19:20:14.136872Z",
     "iopub.status.busy": "2021-02-09T19:20:14.136156Z",
     "iopub.status.idle": "2021-02-09T19:20:26.678566Z",
     "shell.execute_reply": "2021-02-09T19:20:26.679020Z"
    },
    "id": "o33_jgwGm3NV",
    "papermill": {
     "duration": 12.580909,
     "end_time": "2021-02-09T19:20:26.679205",
     "exception": false,
     "start_time": "2021-02-09T19:20:14.098296",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "\n",
    "# Clone the tensorflow models repository if it doesn't already exist\n",
    "if \"models\" in pathlib.Path.cwd().parts:\n",
    "  while \"models\" in pathlib.Path.cwd().parts:\n",
    "    os.chdir('..')\n",
    "elif not pathlib.Path('models').exists():\n",
    "  !git clone --depth 1 https://github.com/cloud-annotations/models\n",
    "\n",
    "!pip install pycocotools==2.0.0 numpy==1.17.5 tensorflow==1.15 tensorflow-hub==0.3.0\n",
    "!pip install tf_slim\n",
    "!pip install lvis\n",
    "!pip install --no-deps tensorflowjs==1.4.0\n",
    "\n",
    "%cd models/research\n",
    "!protoc object_detection/protos/*.proto --python_out=.\n",
    "\n",
    "pwd = os.getcwd()\n",
    "# we need to set both PYTHONPATH for shell scripts and sys.path for python cells\n",
    "sys.path.append(pwd)\n",
    "sys.path.append(os.path.join(pwd, 'slim'))\n",
    "if 'PYTHONPATH' in os.environ:\n",
    "  os.environ['PYTHONPATH'] += f':{pwd}:{pwd}/slim'\n",
    "else:\n",
    "  os.environ['PYTHONPATH'] = f':{pwd}:{pwd}/slim'\n",
    "%cd ../.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legislative-doctrine",
   "metadata": {
    "id": "ISX8k0TfdDHj",
    "papermill": {
     "duration": 0.017993,
     "end_time": "2021-02-09T19:20:26.715653",
     "exception": false,
     "start_time": "2021-02-09T19:20:26.697660",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Generate a Label Map\n",
    "One piece of data the Object Detection API needs is a label map protobuf. The label map associates an integer id to the text representation of the label. The ids are indexed by 1, meaning the first label will have an id of 1 not 0.\n",
    "\n",
    "Here is an example of what a label map looks like:\n",
    "```\n",
    "item {\n",
    "  id: 1\n",
    "  name: 'Cat'\n",
    "}\n",
    "\n",
    "item {\n",
    "  id: 2\n",
    "  name: 'Dog'\n",
    "}\n",
    "\n",
    "item {\n",
    "  id: 3\n",
    "  name: 'Gold Fish'\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "right-landing",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-09T19:20:26.755007Z",
     "iopub.status.busy": "2021-02-09T19:20:26.754111Z",
     "iopub.status.idle": "2021-02-09T19:20:26.757615Z",
     "shell.execute_reply": "2021-02-09T19:20:26.758283Z"
    },
    "id": "nJsKCG3UdDsn",
    "papermill": {
     "duration": 0.025642,
     "end_time": "2021-02-09T19:20:26.758480",
     "exception": false,
     "start_time": "2021-02-09T19:20:26.732838",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Get a list of labels from the annotations.json\n",
    "labels = {}\n",
    "with open(ANNOTATIONS_JSON_PATH) as f:\n",
    "  annotations = json.load(f)\n",
    "  labels = annotations['labels']\n",
    "\n",
    "# Create a file named label_map.pbtxt\n",
    "os.makedirs(DATA_PATH, exist_ok=True)\n",
    "with open(LABEL_MAP_PATH, 'w') as f:\n",
    "  # Loop through all of the labels and write each label to the file with an id\n",
    "  for idx, label in enumerate(labels):\n",
    "    f.write('item {\\n')\n",
    "    f.write(\"\\tname: '{}'\\n\".format(label))\n",
    "    f.write('\\tid: {}\\n'.format(idx + 1)) # indexes must start at 1\n",
    "    f.write('}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "duplicate-police",
   "metadata": {
    "id": "siRNKiuvsz25",
    "papermill": {
     "duration": 0.014855,
     "end_time": "2021-02-09T19:20:26.788320",
     "exception": false,
     "start_time": "2021-02-09T19:20:26.773465",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Generate TFRecords\n",
    "The TensorFlow Object Detection API expects our data to be in the format of TFRecords.\n",
    "\n",
    "The TFRecord format is a collection of serialized feature dicts, one for each image, looking something like this:\n",
    "```\n",
    "{\n",
    "  'image/height': 1800,\n",
    "  'image/width': 2400,\n",
    "  'image/filename': 'image1.jpg',\n",
    "  'image/source_id': 'image1.jpg',\n",
    "  'image/encoded': ACTUAL_ENCODED_IMAGE_DATA_AS_BYTES,\n",
    "  'image/format': 'jpeg',\n",
    "  'image/object/bbox/xmin': [0.7255949630314233, 0.8845598428835489],\n",
    "  'image/object/bbox/xmax': [0.9695875693160814, 1.0000000000000000],\n",
    "  'image/object/bbox/ymin': [0.5820120073891626, 0.1829972290640394],\n",
    "  'image/object/bbox/ymax': [1.0000000000000000, 0.9662484605911330],\n",
    "  'image/object/class/text': (['Cat', 'Dog']),\n",
    "  'image/object/class/label': ([1, 2])\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appropriate-tissue",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-09T19:20:26.833295Z",
     "iopub.status.busy": "2021-02-09T19:20:26.831966Z",
     "iopub.status.idle": "2021-02-09T19:20:36.160104Z",
     "shell.execute_reply": "2021-02-09T19:20:36.160542Z"
    },
    "id": "cAkOvP-gZR1x",
    "papermill": {
     "duration": 9.357212,
     "end_time": "2021-02-09T19:20:36.160724",
     "exception": false,
     "start_time": "2021-02-09T19:20:26.803512",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import json\n",
    "import random\n",
    "\n",
    "import PIL.Image\n",
    "import tensorflow as tf\n",
    "\n",
    "from object_detection.utils import dataset_util\n",
    "from object_detection.utils import label_map_util\n",
    "\n",
    "def create_tf_record(images, annotations, label_map, image_path, output):\n",
    "  # Create a train.record TFRecord file.\n",
    "  with tf.python_io.TFRecordWriter(output) as writer:\n",
    "    # Loop through all the training examples.\n",
    "    for image_name in images:\n",
    "      try:\n",
    "        # Make sure the image is actually a file\n",
    "        img_path = os.path.join(image_path, image_name)   \n",
    "        if not os.path.isfile(img_path):\n",
    "          continue\n",
    "          \n",
    "        # Read in the image.\n",
    "        with tf.gfile.GFile(img_path, 'rb') as fid:\n",
    "          encoded_jpg = fid.read()\n",
    "\n",
    "        # Open the image with PIL so we can check that it's a jpeg and get the image\n",
    "        # dimensions.\n",
    "        encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
    "        image = PIL.Image.open(encoded_jpg_io)\n",
    "        if image.format != 'JPEG':\n",
    "          raise ValueError('Image format not JPEG')\n",
    "\n",
    "        width, height = image.size\n",
    "\n",
    "        # Initialize all the arrays.\n",
    "        xmins = []\n",
    "        xmaxs = []\n",
    "        ymins = []\n",
    "        ymaxs = []\n",
    "        classes_text = []\n",
    "        classes = []\n",
    "\n",
    "        # The class text is the label name and the class is the id. If there are 3\n",
    "        # cats in the image and 1 dog, it may look something like this:\n",
    "        # classes_text = ['Cat', 'Cat', 'Dog', 'Cat']\n",
    "        # classes      = [  1  ,   1  ,   2  ,   1  ]\n",
    "\n",
    "        # For each image, loop through all the annotations and append their values.\n",
    "        for a in annotations[image_name]:\n",
    "          if (\"x\" in a and \"x2\" in a and \"y\" in a and \"y2\" in a):\n",
    "            label = a['label']\n",
    "            xmins.append(a[\"x\"])\n",
    "            xmaxs.append(a[\"x2\"])\n",
    "            ymins.append(a[\"y\"])\n",
    "            ymaxs.append(a[\"y2\"])\n",
    "            classes_text.append(label.encode(\"utf8\"))\n",
    "            classes.append(label_map[label])\n",
    "       \n",
    "        # Create the TFExample.\n",
    "        tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
    "          'image/height': dataset_util.int64_feature(height),\n",
    "          'image/width': dataset_util.int64_feature(width),\n",
    "          'image/filename': dataset_util.bytes_feature(image_name.encode('utf8')),\n",
    "          'image/source_id': dataset_util.bytes_feature(image_name.encode('utf8')),\n",
    "          'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
    "          'image/format': dataset_util.bytes_feature('jpeg'.encode('utf8')),\n",
    "          'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
    "          'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
    "          'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
    "          'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
    "          'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
    "          'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
    "        }))\n",
    "        if tf_example:\n",
    "          # Write the TFExample to the TFRecord.\n",
    "          writer.write(tf_example.SerializeToString())\n",
    "      except ValueError:\n",
    "        print('Invalid example, ignoring.')\n",
    "        pass\n",
    "      except IOError:\n",
    "        print(\"Can't read example, ignoring.\")\n",
    "        pass\n",
    "\n",
    "with open(ANNOTATIONS_JSON_PATH) as f:\n",
    "  annotations = json.load(f)['annotations']\n",
    "  image_files = [image for image in annotations.keys()]\n",
    "  # Load the label map we created.\n",
    "  label_map = label_map_util.get_label_map_dict(LABEL_MAP_PATH)\n",
    "\n",
    "  random.seed(42)\n",
    "  random.shuffle(image_files)\n",
    "  num_train = int(0.7 * len(image_files))\n",
    "  train_examples = image_files[:num_train]\n",
    "  val_examples = image_files[num_train:]\n",
    "\n",
    "  create_tf_record(train_examples, annotations, label_map, CLOUD_ANNOTATIONS_MOUNT, TRAIN_RECORD_PATH)\n",
    "  create_tf_record(val_examples, annotations, label_map, CLOUD_ANNOTATIONS_MOUNT, VAL_RECORD_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparable-silence",
   "metadata": {
    "id": "n6DhYpAS7gX2",
    "papermill": {
     "duration": 0.016427,
     "end_time": "2021-02-09T19:20:36.194706",
     "exception": false,
     "start_time": "2021-02-09T19:20:36.178279",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Download a base model\n",
    "Training a model from scratch can take days and tons of data. We can mitigate this by using a pretrained model checkpoint. Instead of starting from nothing, we can add to what was already learned with our own data.\n",
    "\n",
    "There are several pretrained model checkpoints that can be downloaded from the [model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md).\n",
    "\n",
    "The model we will be training is the SSD MobileNet architecture. SSD MobileNet models have a very small file size and can execute very quickly, compromising little accuracy, which makes it perfect for running in the browser. Additionally, we will be using `quantization`. When we say the model is `quantized` it means instead of using `float32` as the datatype of our numbers we are using `float16` or `int8`.\n",
    "\n",
    "- `float32(PI)` = `3.1415927` 32 bits\n",
    "- `float16(PI)` = `3.14` 16 bits\n",
    "- `int8(PI)` = `3` 8 bits\n",
    "\n",
    "We do this because it can cut our model size down by around a factor of 4! An unquantized version of SSD MobileNet that I trained was `22.3 MB`, but the quantized version was `5.7 MB` that's a `~75%` reduction 🎉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operating-calgary",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-09T19:20:36.233831Z",
     "iopub.status.busy": "2021-02-09T19:20:36.233094Z",
     "iopub.status.idle": "2021-02-09T19:20:43.389889Z",
     "shell.execute_reply": "2021-02-09T19:20:43.390312Z"
    },
    "id": "oHD1Jm0v7jfz",
    "papermill": {
     "duration": 7.179808,
     "end_time": "2021-02-09T19:20:43.390489",
     "exception": false,
     "start_time": "2021-02-09T19:20:36.210681",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "\n",
    "import six.moves.urllib as urllib\n",
    "\n",
    "download_base = 'http://download.tensorflow.org/models/object_detection/'\n",
    "model = MODEL_TYPE + '.tar.gz'\n",
    "tmp = 'tmp/checkpoint.tar.gz'\n",
    "\n",
    "if not (os.path.exists(CHECKPOINT_PATH)):\n",
    "  # Download the checkpoint\n",
    "  opener = urllib.request.URLopener()\n",
    "  opener.retrieve(download_base + model, tmp)\n",
    "\n",
    "  # Extract all the `model.ckpt` files.\n",
    "  with tarfile.open(tmp) as tar:\n",
    "    for member in tar.getmembers():\n",
    "      member.name = os.path.basename(member.name)\n",
    "      if 'model.ckpt' in member.name:\n",
    "        tar.extract(member, path=CHECKPOINT_PATH)\n",
    "\n",
    "  os.remove(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-wales",
   "metadata": {
    "id": "UXlvFvwUHrui",
    "papermill": {
     "duration": 0.015074,
     "end_time": "2021-02-09T19:20:43.421587",
     "exception": false,
     "start_time": "2021-02-09T19:20:43.406513",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model config\n",
    "The final thing we need to do is inject our pipline with the amount of labels we have and where to find the label map, TFRecord and model checkpoint. We also need to change the the batch size, because the default batch size of 128 is too large for Colab to handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laughing-tennis",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-09T19:20:43.458441Z",
     "iopub.status.busy": "2021-02-09T19:20:43.457715Z",
     "iopub.status.idle": "2021-02-09T19:20:43.566850Z",
     "shell.execute_reply": "2021-02-09T19:20:43.567281Z"
    },
    "id": "C8CVExv6HsJS",
    "papermill": {
     "duration": 0.131021,
     "end_time": "2021-02-09T19:20:43.567468",
     "exception": false,
     "start_time": "2021-02-09T19:20:43.436447",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from google.protobuf import text_format\n",
    "\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.utils import label_map_util\n",
    "\n",
    "pipeline_skeleton = 'models/research/object_detection/samples/configs/' + CONFIG_TYPE + '.config'\n",
    "configs = config_util.get_configs_from_pipeline_file(pipeline_skeleton)\n",
    "\n",
    "label_map = label_map_util.get_label_map_dict(LABEL_MAP_PATH)\n",
    "num_classes = len(label_map.keys())\n",
    "meta_arch = configs[\"model\"].WhichOneof(\"model\")\n",
    "\n",
    "override_dict = {\n",
    "  'model.{}.num_classes'.format(meta_arch): num_classes,\n",
    "  'train_config.batch_size': 24,\n",
    "  'train_input_path': TRAIN_RECORD_PATH,\n",
    "  'eval_input_path': VAL_RECORD_PATH,\n",
    "  'train_config.fine_tune_checkpoint': os.path.join(CHECKPOINT_PATH, 'model.ckpt'),\n",
    "  'label_map_path': LABEL_MAP_PATH\n",
    "}\n",
    "\n",
    "configs = config_util.merge_external_params_with_configs(configs, kwargs_dict=override_dict)\n",
    "pipeline_config = config_util.create_pipeline_proto_from_configs(configs)\n",
    "config_util.save_pipeline_config(pipeline_config, DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eastern-warner",
   "metadata": {
    "id": "FNYIZK1xVNAa",
    "papermill": {
     "duration": 0.017126,
     "end_time": "2021-02-09T19:20:43.601542",
     "exception": false,
     "start_time": "2021-02-09T19:20:43.584416",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Start training\n",
    "We can start a training run by calling the `model_main` script, passing:\n",
    "- The location of the `pipepline.config` we created\n",
    "- Where we want to save the model\n",
    "- How many steps we want to train the model (the longer you train, the more potential there is to learn)\n",
    "- The number of evaluation steps (or how often to test the model) gives us an idea of how well the model is doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constitutional-handy",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-09T19:20:43.641584Z",
     "iopub.status.busy": "2021-02-09T19:20:43.640895Z",
     "iopub.status.idle": "2021-02-09T19:22:36.353417Z",
     "shell.execute_reply": "2021-02-09T19:22:36.354095Z"
    },
    "id": "Wv5h2bwBVO0V",
    "papermill": {
     "duration": 112.735562,
     "end_time": "2021-02-09T19:22:36.354417",
     "exception": false,
     "start_time": "2021-02-09T19:20:43.618855",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf $OUTPUT_PATH\n",
    "!python -m object_detection.model_main \\\n",
    "    --pipeline_config_path=$DATA_PATH/pipeline.config \\\n",
    "    --model_dir=$OUTPUT_PATH \\\n",
    "    --num_train_steps=$NUM_TRAIN_STEPS \\\n",
    "    --num_eval_steps=100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imposed-channel",
   "metadata": {
    "id": "AwNtvgtdoB-C",
    "papermill": {
     "duration": 0.034056,
     "end_time": "2021-02-09T19:22:36.422437",
     "exception": false,
     "start_time": "2021-02-09T19:22:36.388381",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Export inference graph\n",
    "After your model has been trained, you might have a few checkpoints available. A checkpoint is usually emitted every 500 training steps. Each checkpoint is a snapshot of your model at that point in training. In the event that a long running training process crashes, you can pick up at the last checkpoint instead of starting from scratch.\n",
    "\n",
    "We need to export a checkpoint to a TensorFlow graph proto in order to actually use it. We use regex to find the checkpoint with the highest training step and export it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "early-footwear",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-09T19:22:36.504685Z",
     "iopub.status.busy": "2021-02-09T19:22:36.503708Z",
     "iopub.status.idle": "2021-02-09T19:22:55.026223Z",
     "shell.execute_reply": "2021-02-09T19:22:55.026920Z"
    },
    "id": "BZgP_FZUoE0d",
    "papermill": {
     "duration": 18.571575,
     "end_time": "2021-02-09T19:22:55.027258",
     "exception": false,
     "start_time": "2021-02-09T19:22:36.455683",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "from object_detection.utils.label_map_util import get_label_map_dict\n",
    "\n",
    "regex = re.compile(r\"model\\.ckpt-([0-9]+)\\.index\")\n",
    "numbers = [int(regex.search(f).group(1)) for f in os.listdir(OUTPUT_PATH) if regex.search(f)]\n",
    "TRAINED_CHECKPOINT_PREFIX = os.path.join(OUTPUT_PATH, 'model.ckpt-{}'.format(max(numbers)))\n",
    "\n",
    "print(f'Using {TRAINED_CHECKPOINT_PREFIX}')\n",
    "\n",
    "!rm -rf $EXPORTED_PATH\n",
    "!python -m object_detection.export_inference_graph \\\n",
    "  --pipeline_config_path=$DATA_PATH/pipeline.config \\\n",
    "  --trained_checkpoint_prefix=$TRAINED_CHECKPOINT_PREFIX \\\n",
    "  --output_directory=$EXPORTED_PATH\n",
    "\n",
    "label_map = get_label_map_dict(LABEL_MAP_PATH)\n",
    "label_array = [k for k in sorted(label_map, key=label_map.get)]\n",
    "\n",
    "with open(os.path.join(EXPORTED_PATH, 'labels.json'), 'w') as f:\n",
    "  json.dump(label_array, f)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Copy of object-detection.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 164.110959,
   "end_time": "2021-02-09T19:22:55.592467",
   "environment_variables": {},
   "exception": null,
   "input_path": "/Users/niko/Desktop/obj/train.ipynb",
   "output_path": "/Users/niko/Desktop/obj/train.ipynb",
   "parameters": {},
   "start_time": "2021-02-09T19:20:11.481508",
   "version": "2.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}